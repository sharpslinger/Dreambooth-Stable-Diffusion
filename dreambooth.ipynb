{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b971cc0",
   "metadata": {
    "id": "7b971cc0"
   },
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee101f7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\ty\\AppData\\Local\\Programs\\Python\\Python310\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/ty/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# BUILD ENV for a runpod preloaded with PyTorch 1.13.1\n",
    "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchtext==0.13.1 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install pytorch_lightning==1.6.5\n",
    "\n",
    "#version shouldn't matter here\n",
    "!pip install omegaconf\n",
    "!pip install einops\n",
    "!pip install test-tube\n",
    "!pip install huggingface_hub\n",
    "!pip install gdown\n",
    "!pip install transformers\n",
    "!pip install kornia\n",
    "\n",
    "\n",
    "!pip install setuptools==59.5.0\n",
    "!pip install pillow==9.0.1\n",
    "!pip install torchmetrics==0.6.0\n",
    "\n",
    "#execute setup.py\n",
    "!pip install -e .\n",
    "\n",
    "!pip install protobuf==3.20.1\n",
    "!pip install -qq diffusers[\"training\"]==0.3.0\n",
    "!pip install ftfy\n",
    "!pip install captionizer==1.0.1\n",
    "!pip install taming-transformers-rom1504 \n",
    "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dc106cb",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download a model from hugging face\n",
    "You can also provide your own v1.* model for training by uploading it and renaming it to \"model.ckpt\".  It should be in the same directory as dreambooth.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afa9dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the 1.5 sd model\n",
    "from IPython.display import clear_output\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    " filename=\"v1-5-pruned.ckpt\"\n",
    ")\n",
    "\n",
    "# Move the downloaded ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} model.ckpt\n",
    "clear_output()\n",
    "print(\"âœ… model.ckpt successfully downloaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd748c24",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download pre-generated regularization images\n",
    "We've created the following image sets\n",
    "\n",
    "`man_euler` - provided by Niko Pueringer (Corridor Digital) - euler @ 40 steps, CFG 7.5\n",
    "`man_unsplash` - pictures from various photographers\n",
    "`300_person_ddim`\n",
    "`person_ddim`\n",
    "`woman_ddim` - provided by David Bielejeski - ddim @ 50 steps, CFG 10.0\n",
    "`person_ddim` is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7EydXCjOV1v",
   "metadata": {
    "id": "e7EydXCjOV1v"
   },
   "outputs": [],
   "source": [
    "#Download Regularization Images\n",
    "\n",
    "dataset=\"person_ddim\" #@param [\"man_euler\", \"man_unsplash\", \"300_person_ddim\", \"person_ddim\", \"woman_ddim\", \"blonde_woman\"]\n",
    "!git clone https://github.com/djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git\n",
    "\n",
    "!mkdir -p regularization_images/{dataset}\n",
    "!mv -v Stable-Diffusion-Regularization-Images-{dataset}/{dataset}/*.* regularization_images/{dataset}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f64e38f6",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Upload your training images\n",
    "Upload 10-20 images of someone to\n",
    "\n",
    "```\n",
    "/workspace/Dreambooth-Stable-Diffusion/training_images\n",
    "```\n",
    "\n",
    "WARNING: Be sure to upload an *even* amount of images, otherwise the training inexplicably stops at 1500 steps.\n",
    "\n",
    "*   2-3 full body\n",
    "*   3-5 upper body\n",
    "*   5-12 close-up on face\n",
    "\n",
    "The images should be:\n",
    "\n",
    "- as close as possible to the kind of images you're trying to make"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad4e50df",
   "metadata": {
    "id": "ad4e50df"
   },
   "source": [
    "## Training\n",
    "\n",
    "If training a person or subject, keep an eye on your project's `logs/{folder}/images/train/samples_scaled_gs-00xxxx` generations.\n",
    "\n",
    "If training a style, keep an eye on your project's `logs/{folder}/images/train/samples_gs-00xxxx` generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
   "metadata": {
    "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# This isn't used for training, just to help you remember what your trained into the model.\n",
    "project_name = \"project_name\"\n",
    "\n",
    "# MAX STEPS\n",
    "# How many steps do you want to train for?\n",
    "max_training_steps = 2000\n",
    "\n",
    "# Match class_word to the category of the regularization images you chose above.\n",
    "class_word = \"person\" # typical uses are \"man\", \"person\", \"woman\"\n",
    "\n",
    "# If you are training a person's face, set this to True\n",
    "i_am_training_a_persons_face = True\n",
    "\n",
    "flip_p_arg = 0.0 if i_am_training_a_persons_face else 0.5\n",
    "\n",
    "# This is the unique token you are incorporating into the stable diffusion model.\n",
    "token = \"firstNameLastName\"\n",
    "\n",
    "# Default config, saves the checkpoint when max_training_steps is reached.\n",
    "config = \"v1-finetune_unfrozen.yaml\"\n",
    "\n",
    "# Uncomment the following line to save checkpoints every 250 steps.\n",
    "#config = \"v1-finetune_unfrozen_save_checkpoints_every_250_steps.yaml\"\n",
    "\n",
    "# Uncomment the following line to save checkpoints every 500 steps.\n",
    "#config = \"v1-finetune_unfrozen_save_checkpoints_every_500_steps.yaml\"\n",
    "\n",
    "reg_data_root = \"/Dreambooth-Stable-Diffusion/regularization_images/\" + dataset\n",
    "\n",
    "!rm -rf training_images/.ipynb_checkpoints\n",
    "!python \"main.py\" \\\n",
    " --base \"configs/stable-diffusion/{config}\" \\\n",
    " -t \\\n",
    " --actual_resume \"model.ckpt\" \\\n",
    " --reg_data_root \"{reg_data_root}\" \\\n",
    " -n \"{project_name}\" \\\n",
    " --gpus 0, \\\n",
    " --data_root \"/Dreambooth-Stable-Diffusion/training_images\" \\\n",
    " --max_training_steps {max_training_steps} \\\n",
    " --class_word \"{class_word}\" \\\n",
    " --token \"{token}\" \\\n",
    " --no-test \\\n",
    " --flip_p {flip_p_arg}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc49d0bd",
   "metadata": {},
   "source": [
    "## Copy and name the checkpoint file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec57ec3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "training_images = !find training_images/*\n",
    "date_string = !date +\"%Y-%m-%dT%H-%M-%S\"\n",
    "\n",
    "if config == \"v1-finetune_unfrozen.yaml\":\n",
    " # Copy the checkpoint into our `trained_models` folder\n",
    " directory_paths = !ls -d logs/*\n",
    " last_checkpoint_file = directory_paths[-1] + \"/checkpoints/last.ckpt\"\n",
    " file_name = date_string[-1] + \"_\" + \\\n",
    "             project_name + \"_\" + \\\n",
    "             str(len(training_images)) + \"_training_images_\" + \\\n",
    "             str(max_training_steps) + \"_max_training_steps_\" + \\\n",
    "             token + \"_token_\" + \\\n",
    "             class_word + \"_class_word.ckpt\"\n",
    "\n",
    " file_name = file_name.replace(\" \", \"_\")\n",
    "\n",
    " !mkdir -p trained_models\n",
    " !mv \"{last_checkpoint_file}\" \"trained_models/{file_name}\"\n",
    "\n",
    " print(\"Download your trained model from trained_models/\" + file_name + \" and use in your favorite Stable Diffusion repo!\")\n",
    "else:\n",
    " directory_paths = !ls -d logs/*\n",
    " checkpoints_directory = directory_paths[-1] + \"/checkpoints/trainstep_checkpoints\"\n",
    " file_paths = !ls -d \"{checkpoints_directory}\"/*\n",
    "\n",
    " for i, original_file_name in enumerate(file_paths):\n",
    "  # Remove the \"epoch=000000-step=0000\" text\n",
    "  steps = re.sub(checkpoints_directory + \"/epoch=\\d{6}-step=0*\", \"\", original_file_name)\n",
    "\n",
    "  # Remove the .ckpt\n",
    "  steps = steps.replace(\".ckpt\", \"\")\n",
    "\n",
    "  # Setup the filename\n",
    "  file_name = date_string[-1] + \"_\" + \\\n",
    "                  project_name + \"_\" + \\\n",
    "                  str(len(training_images)) + \"_training_images_\" + \\\n",
    "                  steps + \"_training_steps_\" + \\\n",
    "                  token + \"_token_\" + \\\n",
    "                  class_word + \"_class_word.ckpt\"\n",
    "\n",
    "  file_name = file_name.replace(\" \", \"_\")\n",
    "\n",
    "  # Make the directory and move the files into it.\n",
    "  !mkdir -p trained_models\n",
    "  !mv \"{original_file_name}\" \"trained_models/{file_name}\"\n",
    "\n",
    " print(\"Download your trained models from the 'trained_models' folder and use in your favorite Stable Diffusion repo!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e86c46982abaa275695d02ce6cea835b645107253403127abba083515495f7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
